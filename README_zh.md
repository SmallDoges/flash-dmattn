<div align="center">
  <img src="./assets/logo.png" alt="SmallDoges" width="100%">
</div>

<div align="center">


[English](./README.md) | **ç®€ä½“ä¸­æ–‡**

</div>


![Flash-DMA Banner](assets/flash_dmattn_banner.png)

Flash-DMA æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„æ³¨æ„åŠ›å®ç°ï¼Œå°† Flash Attention çš„å†…å­˜æ•ˆç‡ä¸åŠ¨æ€æ©ç æ³¨æ„åŠ›çš„ç¨€ç–è®¡ç®—èƒ½åŠ›ç›¸ç»“åˆï¼Œç”¨äºåœ¨ Transformer æ¨¡å‹ä¸­å¤„ç†è¶…é•¿åºåˆ—ã€‚


## ä¸»è¦ç‰¹æ€§

### ğŸ¯ æ ¸å¿ƒå†…æ ¸ä¼˜åŠ¿
- **4D Mask & Bias æ”¯æŒ**: åŸç”Ÿæ”¯æŒ `(batch_size, num_kv_heads, query_len, key_len)` å½¢çŠ¶çš„ attention_mask å’Œ attention_bias å¼ é‡
- **æ™ºèƒ½è®¡ç®—è·³è¿‡**: åŸºäº attention_mask çš„ block-level è‡ªåŠ¨è·³è¿‡æœºåˆ¶ï¼Œå®Œå…¨è·³è¿‡å…¨é›¶ mask åŒºå—çš„è®¡ç®—å’Œå†…å­˜è®¿é—®
- **å®Œæ•´æ¢¯åº¦æ”¯æŒ**: å†…ç½® attention_bias çš„å®Œæ•´æ¢¯åº¦è®¡ç®—è·¯å¾„ï¼Œæ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒ

### ğŸš€ æ€§èƒ½ä¸æ•ˆç‡
- **åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›**: ä¸ºæ¯ä¸ªæŸ¥è¯¢åŠ¨æ€é€‰æ‹©æœ€é‡è¦çš„é”®ï¼Œå°†è®¡ç®—å¤æ‚åº¦ä» $O(N^2)$ é™ä½åˆ° $O(N \cdot w)$ï¼Œå…¶ä¸­ $w \ll N$ï¼Œ æ”¯æŒå¯è®­ç»ƒçš„ç¨€ç–ç»“æ„
- **å†…å­˜æ•ˆç‡**: ä¿æŒ Flash Attention çš„ $O(N)$ å†…å­˜å¤æ‚åº¦ï¼Œæ— éœ€å®ä¾‹åŒ–å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ
- **CUDA æ·±åº¦ä¼˜åŒ–**: è‡ªå®šä¹‰ CUDA å†…æ ¸ï¼Œå«å…±äº«å†…å­˜åˆ«åã€æµæ°´çº¿é¢„å–ã€æŒ‰å—è·³è¿‡ï¼Œå®ç°é«˜ååä¸ä½è®¿å­˜å¼€é”€
- **è¶…é•¿ä¸Šä¸‹æ–‡æ”¯æŒ**: é€šè¿‡åŠ¨æ€æ©ç çª—å£è£å‰ªï¼Œåœ¨ä¿æŒç²¾åº¦çš„å‰æä¸‹æ”¯æ’‘ 128K+ ä»¤ç‰Œçº§åˆ«çš„ä¸Šä¸‹æ–‡å¤„ç†


## æ€§èƒ½

æˆ‘ä»¬å±•ç¤ºäº† Flash-DMA ç›¸å¯¹äºæ ‡å‡† PyTorch SDPA çš„é¢„æœŸåŠ é€Ÿæ•ˆæœã€‚

---

### å‰å‘ä¼ æ’­æ€§èƒ½

ä»¥ä¸‹è¡¨æ ¼æ˜¯æˆ‘ä»¬åœ¨NVIDIA A100-SXM4-80GBä¸Šå¯¹Flash-DMAä¸æ ‡å‡†PyTorch SDPAåœ¨ä¸åŒé…ç½®ä¸‹çš„å‰å‘æ€§èƒ½å¯¹æ¯”æµ‹è¯•ç»“æœã€‚ç»“æœä¸ºé¢„çƒ­ä¸¤æ¬¡, è¿è¡Œä¸‰æ¬¡çš„å¹³å‡å€¼ã€‚

| Mode   | Q len | K len  | Window W | SDPA (ms) | FDMA (ms) | Speedup |
|--------|-------|--------|----------|-----------|-----------|---------|
| Train  | 256   | 256    | 1024     | 0.29      | 0.19      | 1.58x   |
| Train  | 512   | 512    | 1024     | 0.35      | 0.19      | 1.86x   |
| Train  | 1024  | 1024   | 1024     | 0.51      | 0.18      | 2.81x   |
| Train  | 2048  | 2048   | 1024     | 1.04      | 0.18      | 5.68x   |
| Train  | 4096  | 4096   | 1024     | 2.53      | 0.24      | 10.41x  |
| Train  | 8192  | 8192   | 1024     | 9.38      | 0.36      | 25.93x  |
| Train  | 16384 | 16384  | 1024     | 28.39     | 0.81      | 35.25x  |
| Train  | 32768 | 32768  | 1024     | 111.87    | 2.25      | 49.78x  |
| Train  | 32768 | 32768  | 32       | 113.19    | 2.10      | 53.97x  |
| Train  | 32768 | 32768  | 64       | 113.17    | 2.12      | 53.32x  |
| Train  | 32768 | 32768  | 128      | 113.14    | 2.10      | 53.78x  |
| Train  | 32768 | 32768  | 256      | 113.18    | 2.13      | 53.18x  |
| Train  | 32768 | 32768  | 512      | 113.19    | 2.17      | 52.17x  |
| Train  | 32768 | 32768  | 1024     | 113.19    | 2.24      | 50.45x  |
| Train  | 32768 | 32768  | 2048     | 113.15    | 2.39      | 47.35x  |
| Train  | 32768 | 32768  | 4096     | 113.16    | 2.67      | 42.39x  |
| Train  | 32768 | 32768  | 8192     | 113.11    | 3.20      | 35.29x  |
| Train  | 32768 | 32768  | 16384    | 113.15    | 3.97      | 28.51x  |
| Train  | 32768 | 32768  | 32768    | 113.11    | 4.90      | 23.10x  |
| Infer  | 1     | 256    | 1024     | 0.25      | 0.19      | 1.28x   |
| Infer  | 1     | 512    | 1024     | 0.25      | 0.19      | 1.27x   |
| Infer  | 1     | 1024   | 1024     | 0.25      | 0.20      | 1.28x   |
| Infer  | 1     | 2048   | 1024     | 0.25      | 0.20      | 1.24x   |
| Infer  | 1     | 4096   | 1024     | 0.25      | 0.19      | 1.29x   |
| Infer  | 1     | 8192   | 1024     | 0.25      | 0.20      | 1.25x   |
| Infer  | 1     | 16384  | 1024     | 0.25      | 0.19      | 1.29x   |
| Infer  | 1     | 32768  | 1024     | 0.27      | 0.20      | 1.33x   |
| Infer  | 1     | 65536  | 1024     | 0.42      | 0.20      | 2.10x   |
| Infer  | 1     | 131072 | 1024     | 0.72      | 0.20      | 3.65x   |
| Infer  | 1     | 262144 | 1024     | 1.31      | 0.22      | 6.06x   |
| Infer  | 1     | 524288 | 1024     | 2.49      | 0.24      | 10.45x  |
| Infer  | 1     | 524288 | 32       | 2.48      | 0.21      | 11.60x  |
| Infer  | 1     | 524288 | 64       | 2.44      | 0.21      | 11.66x  |
| Infer  | 1     | 524288 | 128      | 2.45      | 0.21      | 11.47x  |
| Infer  | 1     | 524288 | 256      | 2.43      | 0.21      | 11.47x  |
| Infer  | 1     | 524288 | 512      | 2.44      | 0.22      | 10.89x  |
| Infer  | 1     | 524288 | 1024     | 2.44      | 0.24      | 10.31x  |
| Infer  | 1     | 524288 | 2048     | 2.44      | 0.27      | 9.07x   |
| Infer  | 1     | 524288 | 4096     | 2.45      | 0.33      | 7.41x   |
| Infer  | 1     | 524288 | 8192     | 2.44      | 0.35      | 6.93x   |
| Infer  | 1     | 524288 | 16384    | 2.44      | 0.35      | 6.93x   |
| Infer  | 1     | 524288 | 32768    | 2.45      | 0.35      | 6.96x   |
| Infer  | 1     | 524288 | 65536    | 2.44      | 0.35      | 6.88x   |

---

### åå‘ä¼ æ’­æ€§èƒ½

ä»¥ä¸‹è¡¨æ ¼æ˜¯æˆ‘ä»¬åœ¨NVIDIA A100-SXM4-80GBä¸Šå¯¹Flash-DMAä¸æ ‡å‡†PyTorch SDPAåœ¨ä¸åŒé…ç½®ä¸‹çš„åå‘æ€§èƒ½å¯¹æ¯”æµ‹è¯•ç»“æœã€‚ç»“æœä¸ºé¢„çƒ­ä¸¤æ¬¡, è¿è¡Œä¸‰æ¬¡çš„å¹³å‡å€¼ã€‚

| Mode  | Q len | K len  | Window W | SDPA-BWD (ms) | FDMA-BWD (ms) | Speedup |
|-------|-------|--------|----------|---------------|---------------|---------|
| Train | 256   | 256    | 1024     | 0.42          | 0.62          | 0.7x    |
| Train | 512   | 512    | 1024     | 0.56          | 0.60          | 0.9x    |
| Train | 1024  | 1024   | 1024     | 0.94          | 0.61          | 1.5x    |
| Train | 2048  | 2048   | 1024     | 1.79          | 0.69          | 2.6x    |
| Train | 4096  | 4096   | 1024     | 3.76          | 1.08          | 3.5x    |
| Train | 8192  | 8192   | 1024     | 14.39         | 2.06          | 7.0x    |
| Train | 16384 | 16384  | 1024     | 39.56         | 4.97          | 8.0x    |
| Train | 32768 | 32768  | 1024     | 142.07        | 25.63         | 5.5x    |
| Train | 32768 | 32768  | 32       | 142.70        | 21.91         | 6.5x    |
| Train | 32768 | 32768  | 64       | 142.65        | 22.29         | 6.4x    |
| Train | 32768 | 32768  | 128      | 142.69        | 23.04         | 6.2x    |
| Train | 32768 | 32768  | 256      | 142.69        | 24.27         | 5.9x    |
| Train | 32768 | 32768  | 512      | 142.67        | 25.12         | 5.7x    |
| Train | 32768 | 32768  | 1024     | 142.55        | 25.58         | 5.6x    |
| Train | 32768 | 32768  | 2048     | 142.75        | 25.64         | 5.6x    |
| Train | 32768 | 32768  | 4096     | 142.61        | 24.84         | 5.7x    |
| Train | 32768 | 32768  | 8192     | 142.33        | 25.63         | 5.6x    |
| Train | 32768 | 32768  | 16384    | 142.40        | 25.62         | 5.6x    |
| Train | 32768 | 32768  | 32768    | 142.43        | 25.63         | 5.6x    |

---


## å®‰è£…

### å…ˆå†³æ¡ä»¶

- **Python**: 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **PyTorch**: 2.0.0 æˆ–æ›´é«˜ç‰ˆæœ¬  
- **CUDA**: 11.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **NVIDIA GPU**: è®¡ç®—èƒ½åŠ› 8.0 æˆ–æ›´é«˜
- **C++ ç¼–è¯‘å™¨**: GCC 7+

### CUDA ç¯å¢ƒè®¾ç½®

ç¡®ä¿æ‚¨çš„ CUDA ç¯å¢ƒå·²æ­£ç¡®é…ç½®ï¼š

```bash
# æ£€æŸ¥ CUDA å®‰è£…
nvcc --version

# å¦‚éœ€è¦ï¼Œè®¾ç½® CUDA_HOME
export CUDA_HOME=/usr/local/cuda
```

### ä»æºç å®‰è£…

```bash
git clone https://github.com/SmallDoges/flash-dmattn.git
cd flash-dmattn
MAX_JOBS=4 pip install . --no-build-isolation
```


## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ç”¨æ³•

```python
import torch
from flash_dmattn import flash_dmattn_func_auto
import math

# è®¾ç½®
batch_size, seq_len, num_heads, num_kv_heads, head_dim = 1, 256, 2, 1, 64
keep_window_size = 128
device = torch.device('cuda')
dtype = torch.bfloat16
min_dtype = torch.finfo(dtype).min  # dtype çš„æœ€å°å€¼

# è¾“å…¥å¼ é‡
query = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=dtype)
key = torch.randn(batch_size, seq_len, num_kv_heads, head_dim, device=device, dtype=dtype)
value = torch.randn(batch_size, seq_len, num_kv_heads, head_dim, device=device, dtype=dtype)

# ä¸ºç¨€ç–æ³¨æ„åŠ›åˆ›å»º mask å’Œ bias
attention_mask = torch.ones(batch_size, num_kv_heads, seq_len, seq_len, device=device, dtype=dtype)
attention_bias = torch.randn(batch_size, num_kv_heads, seq_len, seq_len, device=device, dtype=dtype)

# åŸºäº bias ç”Ÿæˆç¨€ç– mask
if seq_len > keep_window_size:
    # ä¸ºæ¯ä¸ªæŸ¥è¯¢é€‰æ‹© top-k æœ€é‡è¦çš„é”®
    topk_values, topk_indices = torch.topk(
        attention_bias, keep_window_size, dim=-1, 
        largest=True, sorted=False
    )
    # ç”Ÿæˆæœ‰æ•ˆçš„ top-k mask
    valid_topk = (topk_values != min_dtype).to(dtype)
    attention_mask = torch.zeros_like(attention_bias, dtype=dtype, device=attention_bias.device)
    attention_mask = attention_mask.scatter(-1, topk_indices, valid_topk)
    attention_bias = attention_bias.masked_fill(attention_mask == 0.0, min_dtype)

# é€‰æ‹© FDMA å†…æ ¸
flash_dmattn_func = flash_dmattn_func_auto(backend="cuda")

# è¿è¡Œ FDMA
output = flash_dmattn_func(
    query=query,
    key=key, 
    value=value,
    attn_mask=attention_mask,
    attn_bias=attention_bias,
    is_causal=True,
    scale=1.0/math.sqrt(head_dim),
)

print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [1, 256, 2, 64]
```

### æ¢¯åº¦è®¡ç®—ç¤ºä¾‹

```python
# å¼€å¯æ¢¯åº¦è®¡ç®—
query.requires_grad_(True)
key.requires_grad_(True)
value.requires_grad_(True)
attention_bias.requires_grad_(True)

# å‰å‘ä¼ æ’­
output = flash_dmattn_func(
    query=query, key=key, value=value,
    attn_mask=attention_mask,
    attn_bias=attention_bias,
    is_causal=True,
    scale=1.0/math.sqrt(head_dim)
)

# åå‘ä¼ æ’­
loss = output.sum()
loss.backward()

print(f"Query æ¢¯åº¦å½¢çŠ¶: {query.grad.shape}")
print(f"Key æ¢¯åº¦å½¢çŠ¶: {key.grad.shape}")
print(f"Value æ¢¯åº¦å½¢çŠ¶: {value.grad.shape}")
print(f"Bias æ¢¯åº¦å½¢çŠ¶: {attention_bias.grad.shape}")
```


## å·¥ä½œåŸç†

Flash-DMA é€šè¿‡å°† Flash Attention çš„é«˜æ•ˆå†…å­˜è®¿é—®æ¨¡å¼ä¸åŠ¨æ€æ©ç æ³¨æ„åŠ›çš„ç¨€ç–è®¡ç®—èƒ½åŠ›ç›¸ç»“åˆï¼Œå®ç°äº†é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ã€‚

### æ ¸å¿ƒæŠ€æœ¯èåˆ

- **ğŸ¯ 4D Mask & Bias åŸç”Ÿæ”¯æŒ**: å†…æ ¸ç›´æ¥å¤„ç† `(batch_size, num_kv_heads, query_len, key_len)` å½¢çŠ¶çš„å¼ é‡
- **âš¡ Block-level æ™ºèƒ½è·³è¿‡**: åŸºäº mask çš„ç»Ÿä¸€ OR-reduction è·³è¿‡é€»è¾‘ï¼Œå®Œå…¨é¿å…å…¨é›¶åŒºå—çš„è®¡ç®—å’Œå†…å­˜è®¿é—®
- **ğŸ”„ å®Œæ•´æ¢¯åº¦é“¾è·¯**: å†…ç½® attention bias æ¢¯åº¦è®¡ç®—ï¼Œæ”¯æŒç«¯åˆ°ç«¯å¯å¾®åˆ†è®­ç»ƒ

### å…³é”®ä¼˜åŒ–ç­–ç•¥

1. **ç»Ÿä¸€è·³è¿‡é€»è¾‘**: å‰å‘å’Œåå‘è¿‡ç¨‹ä½¿ç”¨ç›¸åŒçš„ block-level è·³è¿‡å†³ç­–
2. **å†…å­˜è®¿é—®ä¼˜åŒ–**: åªæœ‰å½“ `OR(mask_block) == true` æ—¶æ‰åŠ è½½ K/V æ•°æ®
3. **æ¢¯åº¦è·¯å¾„å®Œæ•´æ€§**: dbias æ¢¯åº¦è®¡ç®—å®Œå…¨èåˆåœ¨åå‘å†…æ ¸ä¸­
4. **å…±äº«å†…å­˜å¤ç”¨**: sMask â†” sP, sBias â†” sdS æ™ºèƒ½åˆ«ååŒ–


## æ–‡æ¡£

ğŸ“š **å®Œæ•´æ–‡æ¡£å¯åœ¨ [docs](docs/) ç›®å½•ä¸­æ‰¾åˆ°ï¼š**

- **[API å‚è€ƒ](docs/api_reference.md)** - å®Œæ•´çš„å‡½æ•°æ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹
- **[é›†æˆæŒ‡å—](docs/integration.md)** - Flash Attention é›†æˆçš„è¯¦ç»†æŠ€æœ¯æ–‡æ¡£


## ä»æºç æ„å»º

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†åŒ…å«å­æ¨¡å—
git clone https://github.com/SmallDoges/flash-dmattn.git
cd flash-dmattn

# åœ¨å¼€å‘æ¨¡å¼ä¸‹æ„å»º
pip install -e .

# è¿è¡Œæµ‹è¯•ä»¥éªŒè¯å®‰è£…
python -c "import flash_dma_cuda; print('âœ… Flash DMA CUDA æ‰©å±•å¯¼å…¥æˆåŠŸ')"
```

### æ„å»ºè¦æ±‚

- CUDA Toolkit 11.8+
- CUTLASS åº“
- æ”¯æŒ CUDA çš„ PyTorch

### æ”¯æŒçš„æ¶æ„

- **SM 8.0** 
- **SM 9.0**
- **SM 10.0**
- **SM 12.0**

**æ³¨æ„**: Flash åŠ¨æ€æ©ç æ³¨æ„åŠ›éœ€è¦ CUDA è®¡ç®—èƒ½åŠ› 8.0+ æ‰èƒ½è·å¾—æœ€ä½³æ€§èƒ½ã€‚ä¸æ”¯æŒæ›´æ—©çš„æ¶æ„ã€‚

## åŸºå‡†æµ‹è¯•

Flash-DMA æä¾›å…¨é¢çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œç”¨äºè¯„ä¼°ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½ï¼š

### å‰å‘ä¼ æ’­ç­‰æ•ˆæ€§
```bash
python benchmarks/forward_equivalence.py
```
éªŒè¯ Python å‚è€ƒå®ç°ä¸ CUDA å®ç°ä¹‹é—´çš„æ•°å€¼ä¸€è‡´æ€§ã€‚

### å‰å‘ä¼ æ’­æ€§èƒ½åŸºå‡†æµ‹è¯•  
```bash
python benchmarks/forward_performance.py
```
åœ¨å„ç§åºåˆ—é•¿åº¦å’Œæ‰¹å¤§å°ä¸‹æ¯”è¾ƒ Flash-DMA ä¸æ ‡å‡† SDPAã€‚

### åå‘ä¼ æ’­ç­‰æ•ˆæ€§
```bash
python benchmarks/backward_equivalence.py
```
éªŒè¯ Python å‚è€ƒå®ç°ä¸ CUDA å®ç°ä¹‹é—´çš„æ•°å€¼ä¸€è‡´æ€§ã€‚

### åå‘ä¼ æ’­æ€§èƒ½åŸºå‡†æµ‹è¯•
```bash
python benchmarks/backward_performance.py
```
æ¯”è¾ƒ Flash-DMA ä¸æ ‡å‡† SDPA åœ¨å„ç§åºåˆ—é•¿åº¦å’Œæ‰¹å¤§å°ä¸‹çš„æ€§èƒ½ã€‚

### æ¢¯åº¦è®¡ç®—
```bash
python benchmarks/grad_equivalence.py
```
æµ‹è¯•åå‘ä¼ æ’­å®ç°å’Œæ¢¯åº¦ç­‰æ•ˆæ€§ã€‚


## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**ç¼–è¯‘é”™è¯¯**
```bash
# ç¡®ä¿ CUDA_HOME è®¾ç½®æ­£ç¡®
echo $CUDA_HOME         # Linux/Mac
echo $env:CUDA_HOME     # Windows PowerShell

# æ£€æŸ¥ CUDA å·¥å…·åŒ…ç‰ˆæœ¬
nvcc --version

# éªŒè¯ PyTorch CUDA æ”¯æŒ
python -c "import torch; print(f'CUDA å¯ç”¨: {torch.cuda.is_available()}')"
```

**å¯¼å…¥é”™è¯¯**
```python
# æµ‹è¯•åŸºæœ¬å¯¼å…¥
try:
    from flash_dmattn import flash_dmattn_func, get_available_backends
    print("âœ… Flash åŠ¨æ€æ©ç æ³¨æ„åŠ›å¯¼å…¥æˆåŠŸ")
    print(f"å¯ç”¨åç«¯: {get_available_backends()}")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åŒ…å·²æ­£ç¡®å®‰è£…ï¼Œä½¿ç”¨: pip install -e .")
```

**æ€§èƒ½é—®é¢˜**
```python
# ç›‘æ§ GPU å†…å­˜ä½¿ç”¨
from flash_dmattn import flash_dmattn_func

def print_memory_stats():
    if torch.cuda.is_available():
        print(f"GPU å†…å­˜: {torch.cuda.memory_allocated() / 1e9:.2f} GB")

print_memory_stats()
output = flash_dmattn_func(q=query, k=key, v=value, is_causal=True)
print_memory_stats()

# å¦‚éœ€è¦ï¼Œæ¸…é™¤ç¼“å­˜
torch.cuda.empty_cache()
```


## è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºçš„è´¡çŒ®ï¼Flash-DMA æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæˆ‘ä»¬é‡è§†æ‰€æœ‰ç±»å‹çš„è´¡çŒ®ã€‚

### å¦‚ä½•è´¡çŒ®

- **æŠ¥å‘Šé”™è¯¯**: å‘ç°äº†é”™è¯¯ï¼Ÿè¯·[æäº¤ issue](https://github.com/SmallDoges/flash-dmattn/issues/new/choose)
- **åŠŸèƒ½è¯·æ±‚**: æœ‰æ”¹è¿›æƒ³æ³•ï¼Ÿ[å‘Šè¯‰æˆ‘ä»¬](https://github.com/SmallDoges/flash-dmattn/issues/new/choose)
- **æäº¤ä»£ç **: å‡†å¤‡è´¡çŒ®ä»£ç ï¼ŸæŸ¥çœ‹æˆ‘ä»¬çš„[è´¡çŒ®æŒ‡å—](CONTRIBUTING.md)
- **æ”¹è¿›æ–‡æ¡£**: å¸®åŠ©æˆ‘ä»¬å®Œå–„æ–‡æ¡£

### è´¡çŒ®è€…å¿«é€Ÿå…¥é—¨

1. Fork ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯: `git checkout -b feature-name`
3. è¿›è¡Œä¿®æ”¹å¹¶æµ‹è¯•
4. æäº¤ Pull Request

è¯¦ç»†è¯´æ˜è¯·å‚è§æˆ‘ä»¬çš„[è´¡çŒ®æŒ‡å—](CONTRIBUTING.md)ã€‚

### è¡Œä¸ºå‡†åˆ™

æœ¬é¡¹ç›®éµå¾ª[è´¡çŒ®è€…å…¬çº¦è¡Œä¸ºå‡†åˆ™](CODE_OF_CONDUCT.md)ã€‚å‚ä¸æ—¶ï¼Œæ‚¨éœ€è¦éµå®ˆæ­¤å‡†åˆ™ã€‚

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ BSD 3-Clause è®¸å¯è¯ã€‚è¯¦æƒ…è¯·å‚è§ [LICENSE](LICENSE)ã€‚

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨ Flash-DMAï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{shi2025trainabledynamicmasksparse,
      title={Trainable Dynamic Mask Sparse Attention}, 
      author={Jingze Shi and Yifan Wu and Bingheng Wu and Yiran Peng and Liangdong Wang and Guang Liu and Yuyu Luo},
      year={2025},
      eprint={2508.02124},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.02124}, 
}
```

## è‡´è°¢

æœ¬é¡¹ç›®åŸºäºå¹¶é›†æˆäº†å‡ ä¸ªä¼˜ç§€çš„å·¥ä½œï¼š

- **[OpenSeek](https://github.com/FlagAI-Open/OpenSeek)** - å†…æ ¸å¼€å‘æ”¯æŒ
- **[Flash-Attention](https://github.com/Dao-AILab/flash-attention)** - å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—
- **[NVIDIA CUTLASS](https://github.com/NVIDIA/cutlass)** - é«˜æ€§èƒ½çŸ©é˜µè¿ç®—åº“

æˆ‘ä»¬æ„Ÿè°¢å¼€æºç¤¾åŒºå¯¹é«˜æ•ˆ Transformer å®ç°çš„è´¡çŒ®ã€‚ğŸ¤—
