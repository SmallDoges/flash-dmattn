<div align="center">
  <img src="./assets/logo.png" alt="SmallDoges" width="100%">
</div>

<div align="center">


[English](./README.md) | **ç®€ä½“ä¸­æ–‡**

</div>

**å¯è®­ç»ƒçš„åŠ¨æ€æ©ç ç¨€ç–æ³¨æ„åŠ›**

> Jingze Shi, Yifan Wu, Bingheng Wu, Yiran Peng, Liangdong Wang, Guang Liu, Yuyu Luo

> è®ºæ–‡: https://huggingface.co/papers/2508.02124

![Flash-DMA Banner](assets/flash_dmattn_banner.png)

Flash-DMA æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„æ³¨æ„åŠ›å®ç°ï¼Œå°† Flash Attention çš„å†…å­˜æ•ˆç‡ä¸åŠ¨æ€æ©ç æ³¨æ„åŠ›çš„ç¨€ç–è®¡ç®—èƒ½åŠ›ç›¸ç»“åˆï¼Œç”¨äºåœ¨ Transformer æ¨¡å‹ä¸­å¤„ç†è¶…é•¿åºåˆ—ã€‚


## ä¸»è¦ç‰¹æ€§

- **ç¨€ç–æ³¨æ„åŠ›è®¡ç®—**: ä¸ºæ¯ä¸ªæŸ¥è¯¢åŠ¨æ€é€‰æ‹©æœ€é‡è¦çš„é”®ï¼Œå°†è®¡ç®—å¤æ‚åº¦ä» $O(N^2)$ é™ä½åˆ° $O(N \cdot w)$ï¼Œå…¶ä¸­ $w \ll N$ã€‚
- **å†…å­˜æ•ˆç‡**: ä¿æŒ Flash Attention çš„ $O(N)$ å†…å­˜å¤æ‚åº¦ï¼Œæ— éœ€å®ä¾‹åŒ–å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µã€‚
- **CUDA åŠ é€Ÿ**: åœ¨ CUDA å†…æ ¸å±‚é¢æ·±åº¦é›†æˆï¼Œé‡‡ç”¨è‡ªå®šä¹‰ç¨€ç– GEMM è¿ç®—ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚
- **é•¿åºåˆ—æ”¯æŒ**: å½“åºåˆ—é•¿åº¦è¶…è¿‡ `keep_window_size` æ—¶ï¼Œé€šè¿‡åŠ¨æ€æ©ç é«˜æ•ˆå¤„ç† 128K+ æ ‡è®°çš„åºåˆ—ã€‚
- **é«˜çº§é›†æˆ**: ä» Python å‰ç«¯åˆ° CUDA åç«¯çš„å®Œæ•´é›†æˆï¼Œå…·æœ‰ä¼˜åŒ–çš„å†…å­˜å¸ƒå±€å’Œç¨€ç–è®¡ç®—ç­–ç•¥ã€‚


## æ€§èƒ½

æˆ‘ä»¬å±•ç¤ºäº† Flash-DMA ç›¸å¯¹äºæ ‡å‡† PyTorch SDPA çš„é¢„æœŸåŠ é€Ÿæ•ˆæœã€‚

![Speedup](assets/speedup.png)


## å®‰è£…

### å…ˆå†³æ¡ä»¶

- **Python**: 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **PyTorch**: 2.0.0 æˆ–æ›´é«˜ç‰ˆæœ¬  
- **CUDA**: 11.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **NVIDIA GPU**: è®¡ç®—èƒ½åŠ› 8.0 æˆ–æ›´é«˜
- **C++ ç¼–è¯‘å™¨**: GCC 7+

### CUDA ç¯å¢ƒè®¾ç½®

ç¡®ä¿æ‚¨çš„ CUDA ç¯å¢ƒå·²æ­£ç¡®é…ç½®ï¼š

```bash
# æ£€æŸ¥ CUDA å®‰è£…
nvcc --version

# å¦‚éœ€è¦ï¼Œè®¾ç½® CUDA_HOME
export CUDA_HOME=/usr/local/cuda
```

### ä»æºç å®‰è£…

```bash
git clone https://github.com/SmallDoges/flash-dmattn.git
cd flash-dmattn
git submodule update --init --recursive
pip install .
```


## å¿«é€Ÿå¼€å§‹

```python
import torch
from flash_dmattn import flash_dmattn_func_auto
import math

# è®¾ç½®
batch_size, seq_len, num_heads, head_dim = 2, 4096, 16, 128
device = torch.device('cuda')
dtype = torch.bfloat16

# è¾“å…¥å¼ é‡
query = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                   device=device, dtype=dtype)
key = torch.randn(batch_size, seq_len, num_heads, head_dim,
                 device=device, dtype=dtype)
value = torch.randn(batch_size, seq_len, num_heads, head_dim,
                   device=device, dtype=dtype)

# ä¸ºç¨€ç–æ³¨æ„åŠ›åˆ›å»ºæ©ç å’Œåç½®
attention_bias = torch.randn(batch_size, num_heads, seq_len, seq_len,
                           device=device, dtype=dtype)
attention_mask = torch.ones(batch_size, num_heads, seq_len, seq_len,
                          device=device, dtype=dtype)

# åº”ç”¨åŠ¨æ€æ©ç ï¼ˆä¸ºé•¿åºåˆ—ä¿ç•™ top-kï¼‰
keep_window_size = 2048
if seq_len > keep_window_size:
    # ä¸ºæ¯ä¸ªæŸ¥è¯¢é€‰æ‹© top-k æœ€é‡è¦çš„é”®
    topk_indices = torch.topk(attention_bias, keep_window_size, dim=-1, 
                             largest=True, sorted=False).indices
    attention_mask.zero_()
    attention_mask.scatter(-1, topk_indices, 1.0)

# é€‰æ‹©åç«¯
flash_dmattn_func = flash_dmattn_func_auto(backend="cuda")

# è¿è¡Œ Flash åŠ¨æ€æ©ç æ³¨æ„åŠ›
output = flash_dmattn_func(
    query=query,
    key=key, 
    value=value,
    attn_mask=attention_mask,
    attn_bias=attention_bias,
    is_causal=True,
    scale=1.0/math.sqrt(head_dim),
)

print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [2, 4096, 16, 128]
```


## å·¥ä½œåŸç†

Flash-DMA ç»“åˆäº†ä¸¤ç§äº’è¡¥çš„æŠ€æœ¯ï¼š

- **åŠ¨æ€æ©ç æ³¨æ„åŠ›**: è®¡ç®—é”®çš„ç›¸å…³æ€§åˆ†æ•°ï¼Œå¹¶ä»…é€‰æ‹©æœ€é‡è¦çš„é”®è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—
- **Flash Attention**: åˆ†å—å¤„ç†æ³¨æ„åŠ›ä»¥å‡å°‘å†…å­˜ä½¿ç”¨å’Œ HBM è®¿é—®

### é›†æˆæ–¹æ³•

é›†æˆå‘ç”Ÿåœ¨ CUDA å†…æ ¸å±‚é¢ï¼Œå…·æœ‰å‡ ä¸ªå…³é”®ç»„ä»¶ï¼š

- **ZOH çŠ¶æ€**: é¢„è®¡ç®—çš„é”®é€‰æ‹©é‡è¦æ€§åˆ†æ•°
- **æ´»è·ƒæ©ç **: æŒ‡ç¤ºæ¯ä¸ªæŸ¥è¯¢åº”è€ƒè™‘å“ªäº›é”®çš„äºŒè¿›åˆ¶æ©ç 
- **ç¨€ç–çŸ©é˜µä¹˜æ³•**: é«˜æ•ˆç¨€ç–æ³¨æ„åŠ›è®¡ç®—çš„è‡ªå®šä¹‰ CUDA å†…æ ¸
- **åˆ†å—å¤„ç†**: ä¿æŒ Flash Attention çš„åˆ†å—æ–¹æ³•ä»¥æé«˜å†…å­˜æ•ˆç‡

è¿™åˆ›å»ºäº†ä¸€ç§æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸ºé•¿åºåˆ—å®ç°äº†å†…å­˜å’Œè®¡ç®—æ•ˆç‡ã€‚


## æ–‡æ¡£

ğŸ“š **å®Œæ•´æ–‡æ¡£å¯åœ¨ [docs](docs/) ç›®å½•ä¸­æ‰¾åˆ°ï¼š**

- **[API å‚è€ƒ](docs/api_reference.md)** - å®Œæ•´çš„å‡½æ•°æ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹
- **[é›†æˆæŒ‡å—](docs/integration.md)** - Flash Attention é›†æˆçš„è¯¦ç»†æŠ€æœ¯æ–‡æ¡£


## ä»æºç æ„å»º

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†åŒ…å«å­æ¨¡å—
git clone --recursive https://github.com/SmallDoges/flash-dmattn.git
cd flash-dmattn

# åœ¨å¼€å‘æ¨¡å¼ä¸‹æ„å»º
pip install -e .

# è¿è¡Œæµ‹è¯•ä»¥éªŒè¯å®‰è£…
python -c "import flash_dma_cuda; print('âœ… Flash DMA CUDA æ‰©å±•å¯¼å…¥æˆåŠŸ')"
```

### æ„å»ºè¦æ±‚

- CUDA Toolkit 11.8+
- CUTLASS åº“
- æ”¯æŒ CUDA çš„ PyTorch

### æ”¯æŒçš„æ¶æ„

- **SM 8.0** 
- **SM 9.0**
- **SM 10.0**
- **SM 12.0**

**æ³¨æ„**: Flash åŠ¨æ€æ©ç æ³¨æ„åŠ›éœ€è¦ CUDA è®¡ç®—èƒ½åŠ› 8.0+ æ‰èƒ½è·å¾—æœ€ä½³æ€§èƒ½ã€‚ä¸æ”¯æŒæ›´æ—©çš„æ¶æ„ã€‚

## åŸºå‡†æµ‹è¯•

Flash-DMA æä¾›å…¨é¢çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œç”¨äºè¯„ä¼°ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½ï¼š

### å‰å‘ä¼ æ’­ç­‰æ•ˆæ€§
```bash
python benchmarks/benchmark_forward_equivalence.py
```
éªŒè¯ Python å‚è€ƒå®ç°ä¸ CUDA å®ç°ä¹‹é—´çš„æ•°å€¼ä¸€è‡´æ€§ã€‚

### æ€§èƒ½åŸºå‡†æµ‹è¯•  
```bash
python benchmarks/benchmark_forward_performance.py
```
åœ¨å„ç§åºåˆ—é•¿åº¦å’Œæ‰¹å¤§å°ä¸‹æ¯”è¾ƒ Flash-DMA ä¸æ ‡å‡† Flash Attentionã€‚

### æ¢¯åº¦è®¡ç®—
```bash
python benchmarks/benchmark_grad.py
```
æµ‹è¯•åå‘ä¼ æ’­å®ç°å’Œæ¢¯åº¦ç­‰æ•ˆæ€§ã€‚

### å¤šæŸ¥è¯¢è”æƒ³å›å¿†
```bash
python benchmarks/benchmark_mqar.py
```
è¯„ä¼°é•¿ç¨‹æ¨ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚


## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**ç¼–è¯‘é”™è¯¯**
```bash
# ç¡®ä¿ CUDA_HOME è®¾ç½®æ­£ç¡®
echo $CUDA_HOME  # Linux/Mac
echo $env:CUDA_HOME  # Windows PowerShell

# æ£€æŸ¥ CUDA å·¥å…·åŒ…ç‰ˆæœ¬
nvcc --version

# éªŒè¯ PyTorch CUDA æ”¯æŒ
python -c "import torch; print(f'CUDA å¯ç”¨: {torch.cuda.is_available()}')"
```

**å¯¼å…¥é”™è¯¯**
```python
# æµ‹è¯•åŸºæœ¬å¯¼å…¥
try:
    from flash_dmattn import flash_dmattn_func, get_available_backends
    print("âœ… Flash åŠ¨æ€æ©ç æ³¨æ„åŠ›å¯¼å…¥æˆåŠŸ")
    print(f"å¯ç”¨åç«¯: {get_available_backends()}")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åŒ…å·²æ­£ç¡®å®‰è£…ï¼Œä½¿ç”¨: pip install -e .")
```

**æ€§èƒ½é—®é¢˜**
```python
# ç›‘æ§ GPU å†…å­˜ä½¿ç”¨
from flash_dmattn import flash_dmattn_func

def print_memory_stats():
    if torch.cuda.is_available():
        print(f"GPU å†…å­˜: {torch.cuda.memory_allocated() / 1e9:.2f} GB")

print_memory_stats()
output = flash_dmattn_func(q=query, k=key, v=value, is_causal=True)
print_memory_stats()

# å¦‚éœ€è¦ï¼Œæ¸…é™¤ç¼“å­˜
torch.cuda.empty_cache()
```

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ BSD 3-Clause è®¸å¯è¯ã€‚è¯¦æƒ…è¯·å‚è§ [LICENSE](LICENSE)ã€‚

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨ Flash-DMAï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{shi2025trainabledynamicmasksparse,
      title={Trainable Dynamic Mask Sparse Attention}, 
      author={Jingze Shi and Yifan Wu and Bingheng Wu and Yiran Peng and Liangdong Wang and Guang Liu and Yuyu Luo},
      year={2025},
      eprint={2508.02124},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.02124}, 
}
```

## è‡´è°¢

æœ¬é¡¹ç›®åŸºäºå¹¶é›†æˆäº†å‡ ä¸ªä¼˜ç§€çš„å·¥ä½œï¼š

- **[OpenSeek](https://github.com/FlagAI-Open/OpenSeek)** - å†…æ ¸å¼€å‘æ”¯æŒ
- **[Flash-Attention](https://github.com/Dao-AILab/flash-attention)** - å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—
- **[NVIDIA CUTLASS](https://github.com/NVIDIA/cutlass)** - é«˜æ€§èƒ½çŸ©é˜µè¿ç®—åº“

æˆ‘ä»¬æ„Ÿè°¢å¼€æºç¤¾åŒºå¯¹é«˜æ•ˆ Transformer å®ç°çš„è´¡çŒ®ã€‚ğŸ¤—
