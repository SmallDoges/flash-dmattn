<div align="center">
  <img src="./assets/logo.png" alt="SmallDoges" width="100%">
</div>

<div align="center">


[English](./README.md) | **ç®€ä½“ä¸­æ–‡**

</div>


![Flash-DMA Banner](assets/flash_dmattn_banner.png)

Flash-DMA æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„æ³¨æ„åŠ›å®ç°ï¼Œå°† Flash Attention çš„å†…å­˜æ•ˆç‡ä¸åŠ¨æ€æ©ç æ³¨æ„åŠ›çš„ç¨€ç–è®¡ç®—èƒ½åŠ›ç›¸ç»“åˆï¼Œç”¨äºåœ¨ Transformer æ¨¡å‹ä¸­å¤„ç†è¶…é•¿åºåˆ—ã€‚


## ä¸»è¦ç‰¹æ€§

- **åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›**: ä¸ºæ¯ä¸ªæŸ¥è¯¢åŠ¨æ€é€‰æ‹©æœ€é‡è¦çš„é”®ï¼Œå°†è®¡ç®—å¤æ‚åº¦ä» $O(N^2)$ é™ä½åˆ° $O(N \cdot w)$ï¼Œå…¶ä¸­ $w \ll N$ï¼Œæ”¯æŒå¯è®­ç»ƒçš„ç¨€ç–ç»“æ„ã€‚
- **å†…å­˜æ•ˆç‡**: ä¿æŒ Flash Attention çš„ $O(N)$ å†…å­˜å¤æ‚åº¦ï¼Œæ— éœ€å®ä¾‹åŒ–å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µã€‚
- **CUDA æ·±åº¦ä¼˜åŒ–**ï¼šä½¿ç”¨è‡ªå®šä¹‰ CUDA Kernel, å«å…±äº«å†…å­˜åˆ«åã€æµæ°´çº¿é¢„å–ã€æŒ‰å—è·³è¿‡, å®ç°é«˜ååä¸ä½è®¿å­˜å¼€é”€ã€‚
- **è¶…é•¿ä¸Šä¸‹æ–‡æ”¯æŒ**ï¼šé€šè¿‡åŠ¨æ€æ©ç çª—å£è£å‰ªï¼Œåœ¨ä¿æŒç²¾åº¦çš„å‰æä¸‹æ”¯æ’‘ 128K+ ä»¤ç‰Œçº§åˆ«çš„ä¸Šä¸‹æ–‡å¤„ç†ã€‚
- **å¯å­¦ä¹ åç½®**ï¼šå†…ç½®å¯å­¦ä¹  attention bias åŠå…¶æ¢¯åº¦åå‘è·¯å¾„ dbiasï¼Œæ— éœ€é¢å¤–å¤–éƒ¨ç®—å­ã€‚
- **èåˆå¼è®­ç»ƒå‹å¥½**ï¼šæ­£å‘ä¸åå‘è¿‡ç¨‹å‡æ”¯æŒ block çº§å…¨é›¶æ©ç è·³è¿‡ï¼Œåœ¨ç¨€ç–åœºæ™¯è¿›ä¸€æ­¥é™ä½è®¡ç®—å¼€é”€ã€‚


## æ€§èƒ½

æˆ‘ä»¬å±•ç¤ºäº† Flash-DMA ç›¸å¯¹äºæ ‡å‡† PyTorch SDPA çš„é¢„æœŸåŠ é€Ÿæ•ˆæœã€‚

---

### å‰å‘ä¼ æ’­æ€§èƒ½

ä»¥ä¸‹è¡¨æ ¼æ˜¯æˆ‘ä»¬åœ¨NVIDIA A100-SXM4-80GBä¸Šå¯¹Flash-DMAä¸æ ‡å‡†PyTorch SDPAåœ¨ä¸åŒé…ç½®ä¸‹çš„å‰å‘æ€§èƒ½å¯¹æ¯”æµ‹è¯•ç»“æœã€‚ç»“æœä¸ºé¢„çƒ­ä¸¤æ¬¡, è¿è¡Œä¸‰æ¬¡çš„å¹³å‡å€¼ã€‚

| Mode   | Q len | K len  | Window W | SDPA (ms) | FDMA (ms) | Speedup |
|--------|-------|--------|----------|-----------|-----------|---------|
| Train  | 256   | 256    | 1024     | 0.29      | 0.19      | 1.58x   |
| Train  | 512   | 512    | 1024     | 0.35      | 0.19      | 1.86x   |
| Train  | 1024  | 1024   | 1024     | 0.51      | 0.18      | 2.81x   |
| Train  | 2048  | 2048   | 1024     | 1.04      | 0.18      | 5.68x   |
| Train  | 4096  | 4096   | 1024     | 2.53      | 0.24      | 10.41x  |
| Train  | 8192  | 8192   | 1024     | 9.38      | 0.36      | 25.93x  |
| Train  | 16384 | 16384  | 1024     | 28.39     | 0.81      | 35.25x  |
| Train  | 32768 | 32768  | 1024     | 111.87    | 2.25      | 49.78x  |
| Train  | 32768 | 32768  | 32       | 113.19    | 2.10      | 53.97x  |
| Train  | 32768 | 32768  | 64       | 113.17    | 2.12      | 53.32x  |
| Train  | 32768 | 32768  | 128      | 113.14    | 2.10      | 53.78x  |
| Train  | 32768 | 32768  | 256      | 113.18    | 2.13      | 53.18x  |
| Train  | 32768 | 32768  | 512      | 113.19    | 2.17      | 52.17x  |
| Train  | 32768 | 32768  | 1024     | 113.19    | 2.24      | 50.45x  |
| Train  | 32768 | 32768  | 2048     | 113.15    | 2.39      | 47.35x  |
| Train  | 32768 | 32768  | 4096     | 113.16    | 2.67      | 42.39x  |
| Train  | 32768 | 32768  | 8192     | 113.11    | 3.20      | 35.29x  |
| Train  | 32768 | 32768  | 16384    | 113.15    | 3.97      | 28.51x  |
| Train  | 32768 | 32768  | 32768    | 113.11    | 4.90      | 23.10x  |
| Infer  | 1     | 256    | 1024     | 0.25      | 0.19      | 1.28x   |
| Infer  | 1     | 512    | 1024     | 0.25      | 0.19      | 1.27x   |
| Infer  | 1     | 1024   | 1024     | 0.25      | 0.20      | 1.28x   |
| Infer  | 1     | 2048   | 1024     | 0.25      | 0.20      | 1.24x   |
| Infer  | 1     | 4096   | 1024     | 0.25      | 0.19      | 1.29x   |
| Infer  | 1     | 8192   | 1024     | 0.25      | 0.20      | 1.25x   |
| Infer  | 1     | 16384  | 1024     | 0.25      | 0.19      | 1.29x   |
| Infer  | 1     | 32768  | 1024     | 0.27      | 0.20      | 1.33x   |
| Infer  | 1     | 65536  | 1024     | 0.42      | 0.20      | 2.10x   |
| Infer  | 1     | 131072 | 1024     | 0.72      | 0.20      | 3.65x   |
| Infer  | 1     | 262144 | 1024     | 1.31      | 0.22      | 6.06x   |
| Infer  | 1     | 524288 | 1024     | 2.49      | 0.24      | 10.45x  |
| Infer  | 1     | 524288 | 32       | 2.48      | 0.21      | 11.60x  |
| Infer  | 1     | 524288 | 64       | 2.44      | 0.21      | 11.66x  |
| Infer  | 1     | 524288 | 128      | 2.45      | 0.21      | 11.47x  |
| Infer  | 1     | 524288 | 256      | 2.43      | 0.21      | 11.47x  |
| Infer  | 1     | 524288 | 512      | 2.44      | 0.22      | 10.89x  |
| Infer  | 1     | 524288 | 1024     | 2.44      | 0.24      | 10.31x  |
| Infer  | 1     | 524288 | 2048     | 2.44      | 0.27      | 9.07x   |
| Infer  | 1     | 524288 | 4096     | 2.45      | 0.33      | 7.41x   |
| Infer  | 1     | 524288 | 8192     | 2.44      | 0.35      | 6.93x   |
| Infer  | 1     | 524288 | 16384    | 2.44      | 0.35      | 6.93x   |
| Infer  | 1     | 524288 | 32768    | 2.45      | 0.35      | 6.96x   |
| Infer  | 1     | 524288 | 65536    | 2.44      | 0.35      | 6.88x   |

---

### åå‘ä¼ æ’­æ€§èƒ½

ä»¥ä¸‹è¡¨æ ¼æ˜¯æˆ‘ä»¬åœ¨NVIDIA A100-SXM4-80GBä¸Šå¯¹Flash-DMAä¸æ ‡å‡†PyTorch SDPAåœ¨ä¸åŒé…ç½®ä¸‹çš„åå‘æ€§èƒ½å¯¹æ¯”æµ‹è¯•ç»“æœã€‚ç»“æœä¸ºé¢„çƒ­ä¸¤æ¬¡, è¿è¡Œä¸‰æ¬¡çš„å¹³å‡å€¼ã€‚

| Mode  | Q len | K len  | Window W | SDPA-BWD (ms) | FDMA-BWD (ms) | Speedup |
|-------|-------|--------|----------|---------------|---------------|---------|
| Train | 256   | 256    | 1024     | 0.42          | 0.62          | 0.7x    |
| Train | 512   | 512    | 1024     | 0.56          | 0.60          | 0.9x    |
| Train | 1024  | 1024   | 1024     | 0.94          | 0.61          | 1.5x    |
| Train | 2048  | 2048   | 1024     | 1.79          | 0.69          | 2.6x    |
| Train | 4096  | 4096   | 1024     | 3.76          | 1.08          | 3.5x    |
| Train | 8192  | 8192   | 1024     | 14.39         | 2.06          | 7.0x    |
| Train | 16384 | 16384  | 1024     | 39.56         | 4.97          | 8.0x    |
| Train | 32768 | 32768  | 1024     | 142.07        | 25.63         | 5.5x    |
| Train | 32768 | 32768  | 32       | 142.70        | 21.91         | 6.5x    |
| Train | 32768 | 32768  | 64       | 142.65        | 22.29         | 6.4x    |
| Train | 32768 | 32768  | 128      | 142.69        | 23.04         | 6.2x    |
| Train | 32768 | 32768  | 256      | 142.69        | 24.27         | 5.9x    |
| Train | 32768 | 32768  | 512      | 142.67        | 25.12         | 5.7x    |
| Train | 32768 | 32768  | 1024     | 142.55        | 25.58         | 5.6x    |
| Train | 32768 | 32768  | 2048     | 142.75        | 25.64         | 5.6x    |
| Train | 32768 | 32768  | 4096     | 142.61        | 24.84         | 5.7x    |
| Train | 32768 | 32768  | 8192     | 142.33        | 25.63         | 5.6x    |
| Train | 32768 | 32768  | 16384    | 142.40        | 25.62         | 5.6x    |
| Train | 32768 | 32768  | 32768    | 142.43        | 25.63         | 5.6x    |

---


## å®‰è£…

### å…ˆå†³æ¡ä»¶

- **Python**: 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **PyTorch**: 2.0.0 æˆ–æ›´é«˜ç‰ˆæœ¬  
- **CUDA**: 11.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **NVIDIA GPU**: è®¡ç®—èƒ½åŠ› 8.0 æˆ–æ›´é«˜
- **C++ ç¼–è¯‘å™¨**: GCC 7+

### CUDA ç¯å¢ƒè®¾ç½®

ç¡®ä¿æ‚¨çš„ CUDA ç¯å¢ƒå·²æ­£ç¡®é…ç½®ï¼š

```bash
# æ£€æŸ¥ CUDA å®‰è£…
nvcc --version

# å¦‚éœ€è¦ï¼Œè®¾ç½® CUDA_HOME
export CUDA_HOME=/usr/local/cuda
```

### ä»æºç å®‰è£…

```bash
git clone https://github.com/SmallDoges/flash-dmattn.git
cd flash-dmattn
MAX_JOBS=4 pip install . --no-build-isolation
```


## å¿«é€Ÿå¼€å§‹

```python
import torch
from flash_dmattn import flash_dmattn_func_auto
import math

# è®¾ç½®
batch_size, seq_len, num_heads, head_dim = 2, 4096, 16, 128
device = torch.device('cuda')
dtype = torch.bfloat16

# è¾“å…¥å¼ é‡
query = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                   device=device, dtype=dtype)
key = torch.randn(batch_size, seq_len, num_heads, head_dim,
                 device=device, dtype=dtype)
value = torch.randn(batch_size, seq_len, num_heads, head_dim,
                   device=device, dtype=dtype)

# ä¸ºç¨€ç–æ³¨æ„åŠ›åˆ›å»ºæ©ç å’Œåç½®
attention_bias = torch.randn(batch_size, num_heads, seq_len, seq_len,
                           device=device, dtype=dtype)
attention_mask = torch.ones(batch_size, num_heads, seq_len, seq_len,
                          device=device, dtype=dtype)

# åº”ç”¨åŠ¨æ€æ©ç ï¼ˆä¸ºé•¿åºåˆ—ä¿ç•™ top-kï¼‰
keep_window_size = 2048
if seq_len > keep_window_size:
    # ä¸ºæ¯ä¸ªæŸ¥è¯¢é€‰æ‹© top-k æœ€é‡è¦çš„é”®
    topk_indices = torch.topk(attention_bias, keep_window_size, dim=-1, 
                             largest=True, sorted=False).indices
    attention_mask.zero_()
    attention_mask.scatter(-1, topk_indices, 1.0)

# é€‰æ‹©åç«¯
flash_dmattn_func = flash_dmattn_func_auto(backend="cuda")

# è¿è¡Œ Flash åŠ¨æ€æ©ç æ³¨æ„åŠ›
output = flash_dmattn_func(
    query=query,
    key=key, 
    value=value,
    attn_mask=attention_mask,
    attn_bias=attention_bias,
    is_causal=True,
    scale=1.0/math.sqrt(head_dim),
)

print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [2, 4096, 16, 128]
```


## å·¥ä½œåŸç†

Flash-DMA ç»“åˆäº†ä¸¤ç§äº’è¡¥çš„æŠ€æœ¯ï¼š

- **åŠ¨æ€æ©ç æ³¨æ„åŠ›**: è®¡ç®—é”®çš„ç›¸å…³æ€§åˆ†æ•°ï¼Œå¹¶ä»…é€‰æ‹©æœ€é‡è¦çš„é”®è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—
- **Flash Attention**: åˆ†å—å¤„ç†æ³¨æ„åŠ›ä»¥å‡å°‘å†…å­˜ä½¿ç”¨å’Œ HBM è®¿é—®

### é›†æˆæ–¹æ³•

é›†æˆå‘ç”Ÿåœ¨ CUDA å†…æ ¸å±‚é¢ï¼Œå…·æœ‰å‡ ä¸ªå…³é”®ç»„ä»¶ï¼š

- **ZOH çŠ¶æ€**: é¢„è®¡ç®—çš„é”®é€‰æ‹©é‡è¦æ€§åˆ†æ•°
- **æ´»è·ƒæ©ç **: æŒ‡ç¤ºæ¯ä¸ªæŸ¥è¯¢åº”è€ƒè™‘å“ªäº›é”®çš„äºŒè¿›åˆ¶æ©ç 
- **ç¨€ç–è·³è¿‡**: é«˜æ•ˆç¨€ç–æ³¨æ„åŠ›è®¡ç®—çš„è‡ªå®šä¹‰ CUDA å†…æ ¸
- **åˆ†å—å¤„ç†**: ä¿æŒ Flash Attention çš„åˆ†å—æ–¹æ³•ä»¥æé«˜å†…å­˜æ•ˆç‡

è¿™åˆ›å»ºäº†ä¸€ç§æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸ºé•¿åºåˆ—å®ç°äº†å†…å­˜å’Œè®¡ç®—æ•ˆç‡ã€‚


## æ–‡æ¡£

ğŸ“š **å®Œæ•´æ–‡æ¡£å¯åœ¨ [docs](docs/) ç›®å½•ä¸­æ‰¾åˆ°ï¼š**

- **[API å‚è€ƒ](docs/api_reference.md)** - å®Œæ•´çš„å‡½æ•°æ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹
- **[é›†æˆæŒ‡å—](docs/integration.md)** - Flash Attention é›†æˆçš„è¯¦ç»†æŠ€æœ¯æ–‡æ¡£


## ä»æºç æ„å»º

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†åŒ…å«å­æ¨¡å—
git clone --recursive https://github.com/SmallDoges/flash-dmattn.git
cd flash-dmattn

# åœ¨å¼€å‘æ¨¡å¼ä¸‹æ„å»º
pip install -e .

# è¿è¡Œæµ‹è¯•ä»¥éªŒè¯å®‰è£…
python -c "import flash_dma_cuda; print('âœ… Flash DMA CUDA æ‰©å±•å¯¼å…¥æˆåŠŸ')"
```

### æ„å»ºè¦æ±‚

- CUDA Toolkit 11.8+
- CUTLASS åº“
- æ”¯æŒ CUDA çš„ PyTorch

### æ”¯æŒçš„æ¶æ„

- **SM 8.0** 
- **SM 9.0**
- **SM 10.0**
- **SM 12.0**

**æ³¨æ„**: Flash åŠ¨æ€æ©ç æ³¨æ„åŠ›éœ€è¦ CUDA è®¡ç®—èƒ½åŠ› 8.0+ æ‰èƒ½è·å¾—æœ€ä½³æ€§èƒ½ã€‚ä¸æ”¯æŒæ›´æ—©çš„æ¶æ„ã€‚

## åŸºå‡†æµ‹è¯•

Flash-DMA æä¾›å…¨é¢çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œç”¨äºè¯„ä¼°ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½ï¼š

### å‰å‘ä¼ æ’­ç­‰æ•ˆæ€§
```bash
python benchmarks/forward_equivalence.py
```
éªŒè¯ Python å‚è€ƒå®ç°ä¸ CUDA å®ç°ä¹‹é—´çš„æ•°å€¼ä¸€è‡´æ€§ã€‚

### å‰å‘ä¼ æ’­æ€§èƒ½åŸºå‡†æµ‹è¯•  
```bash
python benchmarks/forward_performance.py
```
åœ¨å„ç§åºåˆ—é•¿åº¦å’Œæ‰¹å¤§å°ä¸‹æ¯”è¾ƒ Flash-DMA ä¸æ ‡å‡† SDPAã€‚

### åå‘ä¼ æ’­ç­‰æ•ˆæ€§
```bash
python benchmarks/backward_equivalence.py
```
éªŒè¯ Python å‚è€ƒå®ç°ä¸ CUDA å®ç°ä¹‹é—´çš„æ•°å€¼ä¸€è‡´æ€§ã€‚

### åå‘ä¼ æ’­æ€§èƒ½åŸºå‡†æµ‹è¯•
```bash
python benchmarks/backward_performance.py
```
æ¯”è¾ƒ Flash-DMA ä¸æ ‡å‡† SDPA åœ¨å„ç§åºåˆ—é•¿åº¦å’Œæ‰¹å¤§å°ä¸‹çš„æ€§èƒ½ã€‚

### æ¢¯åº¦è®¡ç®—
```bash
python benchmarks/grad_equivalence.py
```
æµ‹è¯•åå‘ä¼ æ’­å®ç°å’Œæ¢¯åº¦ç­‰æ•ˆæ€§ã€‚


## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**ç¼–è¯‘é”™è¯¯**
```bash
# ç¡®ä¿ CUDA_HOME è®¾ç½®æ­£ç¡®
echo $CUDA_HOME  # Linux/Mac
echo $env:CUDA_HOME  # Windows PowerShell

# æ£€æŸ¥ CUDA å·¥å…·åŒ…ç‰ˆæœ¬
nvcc --version

# éªŒè¯ PyTorch CUDA æ”¯æŒ
python -c "import torch; print(f'CUDA å¯ç”¨: {torch.cuda.is_available()}')"
```

**å¯¼å…¥é”™è¯¯**
```python
# æµ‹è¯•åŸºæœ¬å¯¼å…¥
try:
    from flash_dmattn import flash_dmattn_func, get_available_backends
    print("âœ… Flash åŠ¨æ€æ©ç æ³¨æ„åŠ›å¯¼å…¥æˆåŠŸ")
    print(f"å¯ç”¨åç«¯: {get_available_backends()}")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åŒ…å·²æ­£ç¡®å®‰è£…ï¼Œä½¿ç”¨: pip install -e .")
```

**æ€§èƒ½é—®é¢˜**
```python
# ç›‘æ§ GPU å†…å­˜ä½¿ç”¨
from flash_dmattn import flash_dmattn_func

def print_memory_stats():
    if torch.cuda.is_available():
        print(f"GPU å†…å­˜: {torch.cuda.memory_allocated() / 1e9:.2f} GB")

print_memory_stats()
output = flash_dmattn_func(q=query, k=key, v=value, is_causal=True)
print_memory_stats()

# å¦‚éœ€è¦ï¼Œæ¸…é™¤ç¼“å­˜
torch.cuda.empty_cache()
```


## è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºçš„è´¡çŒ®ï¼Flash-DMA æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæˆ‘ä»¬é‡è§†æ‰€æœ‰ç±»å‹çš„è´¡çŒ®ã€‚

### å¦‚ä½•è´¡çŒ®

- **æŠ¥å‘Šé”™è¯¯**: å‘ç°äº†é”™è¯¯ï¼Ÿè¯·[æäº¤ issue](https://github.com/SmallDoges/flash-dmattn/issues/new/choose)
- **åŠŸèƒ½è¯·æ±‚**: æœ‰æ”¹è¿›æƒ³æ³•ï¼Ÿ[å‘Šè¯‰æˆ‘ä»¬](https://github.com/SmallDoges/flash-dmattn/issues/new/choose)
- **æäº¤ä»£ç **: å‡†å¤‡è´¡çŒ®ä»£ç ï¼ŸæŸ¥çœ‹æˆ‘ä»¬çš„[è´¡çŒ®æŒ‡å—](CONTRIBUTING.md)
- **æ”¹è¿›æ–‡æ¡£**: å¸®åŠ©æˆ‘ä»¬å®Œå–„æ–‡æ¡£

### è´¡çŒ®è€…å¿«é€Ÿå…¥é—¨

1. Fork ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯: `git checkout -b feature-name`
3. è¿›è¡Œä¿®æ”¹å¹¶æµ‹è¯•
4. æäº¤ Pull Request

è¯¦ç»†è¯´æ˜è¯·å‚è§æˆ‘ä»¬çš„[è´¡çŒ®æŒ‡å—](CONTRIBUTING.md)ã€‚

### è¡Œä¸ºå‡†åˆ™

æœ¬é¡¹ç›®éµå¾ª[è´¡çŒ®è€…å…¬çº¦è¡Œä¸ºå‡†åˆ™](CODE_OF_CONDUCT.md)ã€‚å‚ä¸æ—¶ï¼Œæ‚¨éœ€è¦éµå®ˆæ­¤å‡†åˆ™ã€‚

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ BSD 3-Clause è®¸å¯è¯ã€‚è¯¦æƒ…è¯·å‚è§ [LICENSE](LICENSE)ã€‚

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨ Flash-DMAï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{shi2025trainabledynamicmasksparse,
      title={Trainable Dynamic Mask Sparse Attention}, 
      author={Jingze Shi and Yifan Wu and Bingheng Wu and Yiran Peng and Liangdong Wang and Guang Liu and Yuyu Luo},
      year={2025},
      eprint={2508.02124},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.02124}, 
}
```

## è‡´è°¢

æœ¬é¡¹ç›®åŸºäºå¹¶é›†æˆäº†å‡ ä¸ªä¼˜ç§€çš„å·¥ä½œï¼š

- **[OpenSeek](https://github.com/FlagAI-Open/OpenSeek)** - å†…æ ¸å¼€å‘æ”¯æŒ
- **[Flash-Attention](https://github.com/Dao-AILab/flash-attention)** - å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—
- **[NVIDIA CUTLASS](https://github.com/NVIDIA/cutlass)** - é«˜æ€§èƒ½çŸ©é˜µè¿ç®—åº“

æˆ‘ä»¬æ„Ÿè°¢å¼€æºç¤¾åŒºå¯¹é«˜æ•ˆ Transformer å®ç°çš„è´¡çŒ®ã€‚ğŸ¤—
