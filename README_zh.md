<!-- <div align="center">
  <img src="./assets/logo.png" alt="flash-algo" width="100%">
</div> -->

<div align="center">


[English](./README.md) | **ç®€ä½“ä¸­æ–‡**

</div>


![Flash-Sparse-Attention Banner](assets/flash_sparse_attention_banner.png)

Flash-Sparse-Attention æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å¯è®­ç»ƒç¨€ç–æ³¨æ„åŠ›å®ç°, å°† Flash Attention çš„å†…å­˜æ•ˆç‡ä¸åŠ¨æ€æ©ç æ³¨æ„åŠ›çš„ç¨€ç–è®¡ç®—èƒ½åŠ›ç›¸ç»“åˆ, ç”¨äºåœ¨ Transformer æ¨¡å‹ä¸­å¤„ç†è¶…é•¿åºåˆ—. 


## ä¸ºä»€ä¹ˆé€‰æ‹© Flash-Sparse-Attention

åœ¨å¤§è§„æ¨¡ Transformer çš„è®­ç»ƒä¸æ¨ç†ä¸­, æ³¨æ„åŠ›çš„ä¸»è¦ç“¶é¢ˆå¹¶ä¸ç›¸åŒ:

- è®­ç»ƒä¾§çš„ç®—åŠ›ç“¶é¢ˆ: å…¨æ³¨æ„åŠ›çš„è®¡ç®—å¤æ‚åº¦éšç€åºåˆ—é•¿åº¦å‘ˆäºŒæ¬¡æ–¹å¢é•¿, ä¸”åå‘ä¼ æ’­éœ€é‡å¤åŒçº§åˆ«è®¡ç®—, æµ·é‡ç®—åŠ›æ¶ˆè€—åœ¨è´¡çŒ®æä½çš„é”®å€¼å¯¹ä¸Š.
- æ¨ç†ä¾§çš„è®¿å­˜ç“¶é¢ˆ: å…¨æ³¨æ„åŠ›éœ€è¦åå¤è¯»å†™ Q, K, V ä¸ä¸­é—´å˜é‡, å¯¹ KV-cache çš„è®¿å­˜æˆä¸ºè®¡ç®—æµç¨‹çš„ä¸»å¯¼, ç®—åŠ›éš¾ä»¥è¢«å……åˆ†åˆ©ç”¨.

å› æ­¤, ä¸€ä¸ªæ›´æ­£ç¡®çš„æ–¹å‘æ˜¯ç¨€ç–æ³¨æ„åŠ›: å¯¹æ¯ä¸ªæŸ¥è¯¢ä»…ä¸ $w$ ä¸ªæœ€ç›¸å…³é”®äº¤äº’, æŠŠè®¡ç®—ä¸è®¿å­˜ä» $O(N^2)$ é™åˆ° $O(N\cdot w)$ï¼Œå…¶ä¸­ $w\ll N$. è‹¥ç¨€ç–æ¨¡å¼èƒ½éšä»»åŠ¡è‡ªé€‚åº”, å°±æœ‰æœºä¼šæ—¢å¿«åˆå‡†, åŒæ—¶è§£å†³è®­ç»ƒä¸æ¨ç†çš„ç“¶é¢ˆ, å…·ä½“è¯·å‚è€ƒè®ºæ–‡ [Trainable Dynamic Mask Sparse Attention](https://arxiv.org/abs/2508.02124).


## ä¸»è¦ç‰¹æ€§

### æ”¯æŒçš„åŠŸèƒ½

- å¸¦æœ‰å› æœæ©ç çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
- ä»»æ„ Q å’Œ KV åºåˆ—é•¿åº¦
- ä»»æ„å¤´æ•°å’Œå°äºç­‰äº256çš„å¤´ç»´åº¦
- åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›å’Œå¤šæŸ¥è¯¢æ³¨æ„åŠ›
- çµæ´»çš„æ©ç ä¸åç½®
- è·³è¿‡æ©ç åŒºåŸŸçš„è®¿å­˜ä¸è®¡ç®—
- åç½®çš„æ¢¯åº¦è®¡ç®—

### æˆ‘ä»¬æƒ³è¦æ”¯æŒçš„åŠŸèƒ½

- åˆ†é¡µæ³¨æ„åŠ›
- TMA, WGMMA å’Œ FP8 ä½ç²¾åº¦
- åºåˆ—å¹¶è¡Œ
- è¿›ä¸€æ­¥æå‡è·³è¿‡è®¿å­˜ä¸è®¡ç®—çš„æ€§èƒ½


## å®‰è£…

### ä¾èµ–

- **Linux**: Ubuntu 22.04 æˆ–æ›´é«˜ç‰ˆæœ¬
- **NVIDIA GPU**: è®¡ç®—èƒ½åŠ› 8.0 æˆ–æ›´é«˜
- **C++ ç¼–è¯‘å™¨**: GCC 7+
- **CUDA**: 11.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **Python**: 3.9 æˆ–æ›´é«˜ç‰ˆæœ¬
- **PyTorch**: 2.5.1 æˆ–æ›´é«˜ç‰ˆæœ¬  

### å®‰è£…

æ‚¨å¯ä»¥é€šè¿‡é¢„ç¼–è¯‘çš„è½®å­å®‰è£… FSAï¼š

```bash
pip install flash-sparse-attn --no-build-isolation
```

æˆ–è€…, æ‚¨å¯ä»¥ä»æºä»£ç ç¼–è¯‘å’Œå®‰è£…ï¼š

```bash
git clone https://github.com/flash-algo/flash-sparse-attn.git
cd flash-sparse-attn
pip install . --no-build-isolation
```


## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ç”¨æ³•

```python
import torch
from flash_sparse_attn import flash_sparse_attn_func_auto
from flash_sparse_attn.utils.mask import create_mask
import math

# è®¾ç½®
batch_size, seq_len, num_heads, num_kv_heads, head_dim = 1, 256, 2, 1, 64
window_size = 128
device = torch.device('cuda')
dtype = torch.bfloat16
min_dtype = torch.finfo(dtype).min  # dtype çš„æœ€å°å€¼

# è¾“å…¥å¼ é‡
query = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=dtype)
key = torch.randn(batch_size, seq_len, num_kv_heads, head_dim, device=device, dtype=dtype)
value = torch.randn(batch_size, seq_len, num_kv_heads, head_dim, device=device, dtype=dtype)

# ä¸ºç¨€ç–æ³¨æ„åŠ›åˆ›å»º bias
attn_bias = torch.randn(batch_size, num_kv_heads, 1, seq_len, device=device, dtype=dtype)

# åŸºäº bias ç”ŸæˆåŠ¨æ€ mask
if seq_len > window_size:
    attn_mask = create_mask(
        attention_bias=attn_bias,
        attention_mask=None,
        batch_size=batch_size,
        query_len=seq_len,
        key_len=seq_len,
        window_size=window_size,
        min_dtype=min_dtype,
    )

# é€‰æ‹© FSA å†…æ ¸
flash_sparse_attn_func = flash_sparse_attn_func_auto(backend="cuda")

# è¿è¡Œ FSA
output = flash_sparse_attn_func(
    query=query,
    key=key, 
    value=value,
    attn_mask=attn_mask,
    attn_bias=attn_bias,
    is_causal=True,
    softmax_scale=1.0/math.sqrt(head_dim),
)

print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [1, 256, 2, 64]
```

### æ¢¯åº¦è®¡ç®—ç¤ºä¾‹

```python
# å¼€å¯æ¢¯åº¦è®¡ç®—
query.requires_grad_(True)
key.requires_grad_(True)
value.requires_grad_(True)
attn_bias.requires_grad_(True)

# å‰å‘ä¼ æ’­
output = flash_sparse_attn_func(
    query=query, key=key, value=value,
    attn_mask=attn_mask,
    attn_bias=attn_bias,
    is_causal=True,
    softmax_scale=1.0/math.sqrt(head_dim)
)

# åå‘ä¼ æ’­
loss = output.sum()
loss.backward()

print(f"Query æ¢¯åº¦å½¢çŠ¶: {query.grad.shape}")
print(f"Key æ¢¯åº¦å½¢çŠ¶: {key.grad.shape}")
print(f"Value æ¢¯åº¦å½¢çŠ¶: {value.grad.shape}")
print(f"Bias æ¢¯åº¦å½¢çŠ¶: {attn_bias.grad.shape}")
```


## æ€§èƒ½

æˆ‘ä»¬å±•ç¤ºäº†å¸¦æœ‰maskä¸biasæ¡ä»¶ä¸‹ FSA ç›¸å¯¹äºæ ‡å‡† PyTorch SDPA çš„é¢„æœŸåŠ é€Ÿæ•ˆæœ. 

![FSA Performance Overview](assets/performance_overview.png)

---

### å‰å‘ä¼ æ’­æ€§èƒ½

ä»¥ä¸‹è¡¨æ ¼æ˜¯æˆ‘ä»¬åœ¨NVIDIA A100-SXM4-80GBä¸Šå¯¹FSAä¸æ ‡å‡†PyTorch SDPAåœ¨ä¸åŒé…ç½®ä¸‹çš„å‰å‘æ€§èƒ½å¯¹æ¯”æµ‹è¯•ç»“æœ. ç»“æœä¸ºé¢„çƒ­ä¸¤æ¬¡, è¿è¡Œä¸‰æ¬¡çš„å¹³å‡å€¼. 

| Mode   | Q len | K len  | Window W | SDPA (ms) | FSA (ms) | Speedup |
|--------|-------|--------|----------|-----------|-----------|---------|
| Train  | 256   | 256    | 1024     | 0.29      | 0.19      | 1.58x   |
| Train  | 512   | 512    | 1024     | 0.35      | 0.19      | 1.86x   |
| Train  | 1024  | 1024   | 1024     | 0.51      | 0.18      | 2.81x   |
| Train  | 2048  | 2048   | 1024     | 1.04      | 0.18      | 5.68x   |
| Train  | 4096  | 4096   | 1024     | 2.53      | 0.24      | 10.41x  |
| Train  | 8192  | 8192   | 1024     | 9.38      | 0.36      | 25.93x  |
| Train  | 16384 | 16384  | 1024     | 28.39     | 0.81      | 35.25x  |
| Train  | 32768 | 32768  | 1024     | 111.87    | 2.25      | 49.78x  |
| Train  | 32768 | 32768  | 32       | 113.19    | 2.10      | 53.97x  |
| Train  | 32768 | 32768  | 64       | 113.17    | 2.12      | 53.32x  |
| Train  | 32768 | 32768  | 128      | 113.14    | 2.10      | 53.78x  |
| Train  | 32768 | 32768  | 256      | 113.18    | 2.13      | 53.18x  |
| Train  | 32768 | 32768  | 512      | 113.19    | 2.17      | 52.17x  |
| Train  | 32768 | 32768  | 1024     | 113.19    | 2.24      | 50.45x  |
| Train  | 32768 | 32768  | 2048     | 113.15    | 2.39      | 47.35x  |
| Train  | 32768 | 32768  | 4096     | 113.16    | 2.67      | 42.39x  |
| Train  | 32768 | 32768  | 8192     | 113.11    | 3.20      | 35.29x  |
| Train  | 32768 | 32768  | 16384    | 113.15    | 3.97      | 28.51x  |
| Train  | 32768 | 32768  | 32768    | 113.11    | 4.90      | 23.10x  |
| Infer  | 1     | 256    | 1024     | 0.25      | 0.19      | 1.28x   |
| Infer  | 1     | 512    | 1024     | 0.25      | 0.19      | 1.27x   |
| Infer  | 1     | 1024   | 1024     | 0.25      | 0.20      | 1.28x   |
| Infer  | 1     | 2048   | 1024     | 0.25      | 0.20      | 1.24x   |
| Infer  | 1     | 4096   | 1024     | 0.25      | 0.19      | 1.29x   |
| Infer  | 1     | 8192   | 1024     | 0.25      | 0.20      | 1.25x   |
| Infer  | 1     | 16384  | 1024     | 0.25      | 0.19      | 1.29x   |
| Infer  | 1     | 32768  | 1024     | 0.27      | 0.20      | 1.33x   |
| Infer  | 1     | 65536  | 1024     | 0.42      | 0.20      | 2.10x   |
| Infer  | 1     | 131072 | 1024     | 0.72      | 0.20      | 3.65x   |
| Infer  | 1     | 262144 | 1024     | 1.31      | 0.22      | 6.06x   |
| Infer  | 1     | 524288 | 1024     | 2.49      | 0.24      | 10.45x  |
| Infer  | 1     | 524288 | 32       | 2.48      | 0.21      | 11.60x  |
| Infer  | 1     | 524288 | 64       | 2.44      | 0.21      | 11.66x  |
| Infer  | 1     | 524288 | 128      | 2.45      | 0.21      | 11.47x  |
| Infer  | 1     | 524288 | 256      | 2.43      | 0.21      | 11.47x  |
| Infer  | 1     | 524288 | 512      | 2.44      | 0.22      | 10.89x  |
| Infer  | 1     | 524288 | 1024     | 2.44      | 0.24      | 10.31x  |
| Infer  | 1     | 524288 | 2048     | 2.44      | 0.27      | 9.07x   |
| Infer  | 1     | 524288 | 4096     | 2.45      | 0.33      | 7.41x   |
| Infer  | 1     | 524288 | 8192     | 2.44      | 0.35      | 6.93x   |
| Infer  | 1     | 524288 | 16384    | 2.44      | 0.35      | 6.93x   |
| Infer  | 1     | 524288 | 32768    | 2.45      | 0.35      | 6.96x   |
| Infer  | 1     | 524288 | 65536    | 2.44      | 0.35      | 6.88x   |

---

### åå‘ä¼ æ’­æ€§èƒ½

ä»¥ä¸‹è¡¨æ ¼æ˜¯æˆ‘ä»¬åœ¨NVIDIA A100-SXM4-80GBä¸Šå¯¹FSAä¸æ ‡å‡†PyTorch SDPAåœ¨ä¸åŒé…ç½®ä¸‹çš„åå‘æ€§èƒ½å¯¹æ¯”æµ‹è¯•ç»“æœ. ç»“æœä¸ºé¢„çƒ­ä¸¤æ¬¡, è¿è¡Œä¸‰æ¬¡çš„å¹³å‡å€¼. 

| Mode  | Q len | K len  | Window W | SDPA-BWD (ms) | FSA-BWD (ms) | Speedup |
|-------|-------|--------|----------|---------------|---------------|---------|
| Train | 256   | 256    | 1024     | 0.42          | 0.62          | 0.7x    |
| Train | 512   | 512    | 1024     | 0.56          | 0.60          | 0.9x    |
| Train | 1024  | 1024   | 1024     | 0.94          | 0.61          | 1.5x    |
| Train | 2048  | 2048   | 1024     | 1.79          | 0.69          | 2.6x    |
| Train | 4096  | 4096   | 1024     | 3.76          | 1.08          | 3.5x    |
| Train | 8192  | 8192   | 1024     | 14.39         | 2.06          | 7.0x    |
| Train | 16384 | 16384  | 1024     | 39.56         | 4.97          | 8.0x    |
| Train | 32768 | 32768  | 1024     | 142.07        | 25.63         | 5.5x    |
| Train | 32768 | 32768  | 32       | 142.70        | 21.91         | 6.5x    |
| Train | 32768 | 32768  | 64       | 142.65        | 22.29         | 6.4x    |
| Train | 32768 | 32768  | 128      | 142.69        | 23.04         | 6.2x    |
| Train | 32768 | 32768  | 256      | 142.69        | 24.27         | 5.9x    |
| Train | 32768 | 32768  | 512      | 142.67        | 25.12         | 5.7x    |
| Train | 32768 | 32768  | 1024     | 142.55        | 25.58         | 5.6x    |
| Train | 32768 | 32768  | 2048     | 142.75        | 25.64         | 5.6x    |
| Train | 32768 | 32768  | 4096     | 142.61        | 24.84         | 5.7x    |
| Train | 32768 | 32768  | 8192     | 142.33        | 25.63         | 5.6x    |
| Train | 32768 | 32768  | 16384    | 142.40        | 25.62         | 5.6x    |
| Train | 32768 | 32768  | 32768    | 142.43        | 25.63         | 5.6x    |

---


## åŸºå‡†æµ‹è¯•

FSA æä¾›å…¨é¢çš„åŸºå‡†æµ‹è¯•å·¥å…·, ç”¨äºè¯„ä¼°ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½ï¼š
### å‰å‘ä¼ æ’­ç­‰æ•ˆæ€§
```bash
python benchmarks/forward_equivalence.py
```
éªŒè¯ Python å‚è€ƒå®ç°ä¸ CUDA å®ç°ä¹‹é—´çš„æ•°å€¼ä¸€è‡´æ€§. 

### å‰å‘ä¼ æ’­æ€§èƒ½åŸºå‡†æµ‹è¯•  
```bash
python benchmarks/forward_performance.py
```
åœ¨å„ç§åºåˆ—é•¿åº¦å’Œæ‰¹å¤§å°ä¸‹æ¯”è¾ƒ FSA ä¸æ ‡å‡† SDPA. 

### åå‘ä¼ æ’­ç­‰æ•ˆæ€§
```bash
python benchmarks/backward_equivalence.py
```
éªŒè¯ Python å‚è€ƒå®ç°ä¸ CUDA å®ç°ä¹‹é—´çš„æ•°å€¼ä¸€è‡´æ€§. 

### åå‘ä¼ æ’­æ€§èƒ½åŸºå‡†æµ‹è¯•
```bash
python benchmarks/backward_performance.py
```
æ¯”è¾ƒ FSA ä¸æ ‡å‡† SDPA åœ¨å„ç§åºåˆ—é•¿åº¦å’Œæ‰¹å¤§å°ä¸‹çš„æ€§èƒ½. 

### æ¢¯åº¦è®¡ç®—
```bash
python benchmarks/grad_equivalence.py
```
æµ‹è¯•åå‘ä¼ æ’­å®ç°å’Œæ¢¯åº¦ç­‰æ•ˆæ€§. 


## æ–‡æ¡£

ğŸ“š **å®Œæ•´æ–‡æ¡£å¯åœ¨ [docs](docs/) ç›®å½•ä¸­æ‰¾åˆ°ï¼š**

- **[API å‚è€ƒ](docs/api_reference.md)** - å®Œæ•´çš„å‡½æ•°æ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹


## è´¡çŒ®
    
æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºçš„è´¡çŒ®ï¼FSA æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®, æˆ‘ä»¬é‡è§†æ‰€æœ‰ç±»å‹çš„è´¡çŒ®. 

### å¦‚ä½•è´¡çŒ®

- **æŠ¥å‘Šé”™è¯¯**: å‘ç°äº†é”™è¯¯ï¼Ÿè¯·[æäº¤ issue](https://github.com/flash-algo/flash_sparse_attn/issues/new/choose)
- **åŠŸèƒ½è¯·æ±‚**: æœ‰æ”¹è¿›æƒ³æ³•ï¼Ÿ[å‘Šè¯‰æˆ‘ä»¬](https://github.com/flash-algo/flash_sparse_attn/issues/new/choose)
- **æäº¤ä»£ç **: å‡†å¤‡è´¡çŒ®ä»£ç ï¼ŸæŸ¥çœ‹æˆ‘ä»¬çš„[è´¡çŒ®æŒ‡å—](CONTRIBUTING.md)
- **æ”¹è¿›æ–‡æ¡£**: å¸®åŠ©æˆ‘ä»¬å®Œå–„æ–‡æ¡£

### è´¡çŒ®è€…å¿«é€Ÿå…¥é—¨

1. Fork ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯: `git checkout -b feature-name`
3. è¿›è¡Œä¿®æ”¹å¹¶æµ‹è¯•
4. æäº¤ Pull Request

è¯¦ç»†è¯´æ˜è¯·å‚è§æˆ‘ä»¬çš„[è´¡çŒ®æŒ‡å—](CONTRIBUTING.md). 

### è¡Œä¸ºå‡†åˆ™

æœ¬é¡¹ç›®éµå¾ª[è´¡çŒ®è€…å…¬çº¦è¡Œä¸ºå‡†åˆ™](CODE_OF_CONDUCT.md). å‚ä¸æ—¶, æ‚¨éœ€è¦éµå®ˆæ­¤å‡†åˆ™. 

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ BSD 3-Clause è®¸å¯è¯. è¯¦æƒ…è¯·å‚è§ [LICENSE](LICENSE). 

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨ FSA, è¯·å¼•ç”¨ï¼š

```bibtex
@misc{shi2025trainabledynamicmasksparse,
      title={Trainable Dynamic Mask Sparse Attention}, 
      author={Jingze Shi and Yifan Wu and Bingheng Wu and Yiran Peng and Liangdong Wang and Guang Liu and Yuyu Luo},
      year={2025},
      eprint={2508.02124},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.02124}, 
}
```

## è‡´è°¢

æœ¬é¡¹ç›®åŸºäºå¹¶é›†æˆäº†å‡ ä¸ªä¼˜ç§€çš„å·¥ä½œï¼š

- **[OpenSeek](https://github.com/FlagAI-Open/OpenSeek)** - å†…æ ¸å¼€å‘æ”¯æŒ
- **[Flash-Attention](https://github.com/Dao-AILab/flash-attention)** - å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—
- **[NVIDIA CUTLASS](https://github.com/NVIDIA/cutlass)** - é«˜æ€§èƒ½çŸ©é˜µè¿ç®—åº“

æˆ‘ä»¬æ„Ÿè°¢å¼€æºç¤¾åŒºå¯¹é«˜æ•ˆ Transformer å®ç°çš„è´¡çŒ®. ğŸ¤—
