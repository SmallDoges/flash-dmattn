--- a/csrc/src/flash_attention_fwd_kernel.h
+++ b/csrc/src/flash_attention_fwd_kernel.h
@@ -454,7 +454,11 @@ inline __device__ void compute_attn_1rowblock(const Params &params, const int bi
                         auto mask_values_row = sDynamicMaskValues(m_idx, _);
                         auto predicate_k_row = sPredicate(m_idx, _);
                         if (predicate_k_row(k_idx)) {
-                            acc_s(mma, mi, ki) += static_cast<ElementAccum>(mask_values_row(k_idx));
+                            // Scale the attention score before adding mask value, matching Python's behavior
+                            acc_s(mma, mi, ki) = acc_s(mma, mi, ki) * params.scale_softmax + static_cast<ElementAccum>(mask_values_row(k_idx));
+                        } else {
+                            // For positions where mask is 0, set attention score to -INFINITY so they don't contribute to softmax
+                            acc_s(mma, mi, ki) = -INFINITY;
                         }
                     }
                 }
@@ -567,7 +571,11 @@ inline __device__ void compute_attn_1rowblock(const Params &params, const int bi
                         auto mask_values_row = sDynamicMaskValues(m_idx, _);
                         auto predicate_k_row = sPredicate(m_idx, _);
                         if (predicate_k_row(k_idx)) {
-                            acc_s(mma, mi, ki) += static_cast<ElementAccum>(mask_values_row(k_idx));
+                            // Scale the attention score before adding mask value, matching Python's behavior
+                            acc_s(mma, mi, ki) = acc_s(mma, mi, ki) * params.scale_softmax + static_cast<ElementAccum>(mask_values_row(k_idx));
+                        } else {
+                            // For positions where mask is 0, set attention score to -INFINITY so they don't contribute to softmax
+                            acc_s(mma, mi, ki) = -INFINITY;
                         }
                     }
                 }
